{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 1 - Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instructions`\n",
    "- Convert snappy compressed avro data-files stored at hdfs location /user/cloudera/practice1/question3  into parquet file.\n",
    "- open pyspark/spark-shell with --packages org.apache.spark:spark-avro_2.12:2.4.5\n",
    "\n",
    "\n",
    "\n",
    "`Output Requirement`\n",
    "\n",
    "- Result should be saved in /user/cloudera/practice1/question3/output/\n",
    "- Output should consist of only order_id,order_status\n",
    "- Output file should be saved as Parquet file in gzip Compression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data as dataframe\n",
    "orders = spark.read.format(\"avro\").load(\"/user/cloudera/practice1/question3\")\n",
    "\n",
    "#save the dataframe\n",
    "orders.select(\"order_id\",\"order_status\").write.option(\"compression\",\"gzip\") \\\n",
    ".format(\"parquet\").save(\"/user/cloudera/practice1/question3/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 1 - Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instructions`\n",
    "- Join the comma separated file located at hdfs location /user/cloudera/p1/p4/orders & /user/cloudera/p1/p4/customers to find out  customers who have placed more than 4 orders.\n",
    "\n",
    "\n",
    "\n",
    "`Output Requirement`\n",
    "- Order status should be COMPLETE\n",
    "- Output should have customer_id,customer_fname,count\n",
    "- Save the results in json format.\n",
    "- Result should be order by count of orders in ascending fashion.\n",
    "- Result should be saved in /user/cloudera/p1/p4/output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data as dataframe\n",
    "customers = spark.read.format(\"csv\").load(\"/user/cloudera/p1/p4/customers\")\n",
    "orders = spark.read.format(\"csv\").load(\"/user/cloudera/p1/p4/orders\")\n",
    "\n",
    "#rename columns\n",
    "customers = customers.selectExpr(\"_c0 as customer_id\", \"_c1 as customer_fname\")\n",
    "orders = orders.selectExpr(\"_c2 as customer_id\", \"_c3 as order_status\")\n",
    "\n",
    "#create temp view\n",
    "orders.createorReplaceTempView(\"orders\")\n",
    "\n",
    "#run the sql query\n",
    "customersGroupByCustomerId = spark.sql(\"select customer_id, count(*) as count from orders \\\n",
    "                                       group by customer_id having count>4 order by count desc\")\n",
    "\n",
    "#join two dataframes\n",
    "customersGroupByCustomerId.join(customers, \"customer_id\").write.json(\"/user/cloudera/p1/p4/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 1 - Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instructions`\n",
    "- From provided parquet files located at hdfs location : /user/cloudera/practice1/problem5\n",
    "- Get maximum product_price in each product_category\n",
    "- order the results by maximum price descending.\n",
    "\n",
    "\n",
    "`Output Requirement`\n",
    "- Final output should be saved in below hdfs location: /user/cloudera/practice1/problem5/output\n",
    "- Final output should have product_category_id and max_price separated by pipe delimiter\n",
    "- Ouput should be saved in text format with Gzip compression\n",
    "- Output should be stored in a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data as dataframe\n",
    "products = spark.read.format(\"parquet\").load(\"/user/cloudera/practice1/problem5/\")\n",
    "\n",
    "#create temp view\n",
    "products.createorReplaceTempView(\"products\")\n",
    "\n",
    "#run the sql query\n",
    "productsMaxPricePerCategoryId = spark.sql(\"select concat(product_category_id,'|', max(product_price)) from products \\\n",
    "                                       group by product_category_id order by max(product_price) desc\")\n",
    "#save the dataframe\n",
    "productsMaxPricePerCategoryId.coalesce(1).write.option(\"compression\",\"gzip\").format(\"text\").save(\"/user/cloudera/practice1/problem5/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 1 - Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instructions`\n",
    "- Provided customer tab delimited files at below HDFS location.\n",
    "- Input folder is  /user/cloudera/practice1/problem6\n",
    "- Find all customers that lives 'Caguas' city.\n",
    "\n",
    "\n",
    "`Output Requirement`\n",
    "- Result should be saved in /user/cloudera/practice1/problem6/output\n",
    "- Output file should be saved in avro format in deflate compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data as dataframe\n",
    "customers = spark.read.option(\"sep\",\"\\t\").format(\"csv\").load(\"/user/cloudera/practice1/problem6/\")\n",
    "\n",
    "#create temp view\n",
    "customers.createorReplaceTempView(\"customers\")\n",
    "\n",
    "#run the sql query\n",
    "customersCaguas = spark.sql(\"select * from customers where customer_city = 'Caguas'\")\n",
    "                            \n",
    "#save the dataframe\n",
    "customersCaguas.write.option(\"compression\",\"deflate\").format(\"avro\").save(\"/user/cloudera/practice1/problem6/output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
