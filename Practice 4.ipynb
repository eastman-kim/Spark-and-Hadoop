{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 3 - Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instructions`\n",
    "- From the provided avro files at below HDFS location\n",
    "\t- /user/hive/warehouse/orders\n",
    "\t- /user/hive/warehouse/customers\n",
    "- Find out customers who have not placed any order in March 2013.\n",
    "\n",
    "\n",
    "`Output Requirement`\n",
    "- Output should be stored in json format at below HDFS location /user/cca/practice3/ques1\n",
    "- Output should have two fields customer_fname:customer_lname and customer_city:customer_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into HDFS\n",
    "sqoop import  --connect \"jdbc:mysql://localhost/retail_db\" --username root --password admin --table orders  --warehouse-dir \"/user/hive/warehouse/\" --compress --compression-codec snappy --as-avrodatafile\n",
    "sqoop import  --connect \"jdbc:mysql://localhost/retail_db\" --username root --password admin --table customers  --warehouse-dir \"/user/hive/warehouse/\" --compress --compression-codec snappy --as-avrodatafile \n",
    "\n",
    "#load dataset\n",
    "orders = spark.read.option(\"sep\",\"\\t\").format(\"csv\").load(\"/user/cloudera/practice2/problem3/orders\")\n",
    "orderItems = spark.read.option(\"sep\",\"\\t\").format(\"csv\").load(\"/user/cloudera/practice2/problem3/order_items\")\n",
    "customers = spark.read.option(\"sep\",\"\\t\").format(\"csv\").load(\"/user/cloudera/practice2/problem3/customers\")\n",
    "\n",
    "#rename column names\n",
    "orders = orders.selectExpr(\"_c0 as order_id\", \"_c2 as customer_id\")\n",
    "orderItems = orderItems.selectExpr(\"_c1 as order_id\", \"_c4 as order_item_subtotal\")\n",
    "customers = customers.selectExpr(\"_c0 as customer_id\", \"_c1 as customer_fname\", \"_c2 as customer_lname\", \"_c6 as customer_city\")\n",
    "\n",
    "#run spark sql queries\n",
    "orderItems.createOrReplaceTempView(\"orderItems\")\n",
    "orderItemsFiltered = spark.sql(\"select order_id, sum(order_item_subtotal) as order_amount from orderItems group by order_id having sum(order_item_subtotal)>200\")\n",
    "\n",
    "#join dataframes\n",
    "o_oi_join = orders.join(orderItemsFiltered, \"order_id\")\n",
    "result = o_oi_join.join(customers, \"customer_id\")\n",
    "final = result.selectExpr(\"customer_fname as fname\", \"customer_lname as lname\", \"customer_city as city\", \"order_amount as price\")\n",
    "\n",
    "#save to HDFS\n",
    "final.write.option(\"sep\",\",\").format(\"csv\").save(\"/user/cloudera/practice2/problem3/joinResults\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 3- Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instructions`\n",
    "- Change permissions of all the files under /user/cloudera/problem3/customer/permissions such that owner has read,write and execute permissions, group has read and write permissions whereas others have just read and execute permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop import --connect \"jdbc:mysql://localhost/retail_db\" --password admin --username root --table customers --target-dir /user/cloudera/problem3/customer/permissions\n",
    "\n",
    "hdfs dfs -chmod 765 /user/cloudera/problem3/customer/permissions/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 3 - Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instructions`\n",
    "- Get count of customers in each city who have placed order of amount more than 100 and  whose order status is not PENDING.\n",
    "- Input files are tab delimeted files placed at below HDFS location:\n",
    "\t- /user/cloudera/practice3/problem3/customers\n",
    "\t- /user/cloudera/practice3/problem3/orders\n",
    "\t- /user/cloudera/practice3/problem3/order_items\n",
    "\n",
    "`Output Requirement`\n",
    "- Output should be placed in below HDFS Location /user/cloudera/practice3/problem3/joinResults\n",
    "- Output file should be tab separated file with deflate compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into HDFS\n",
    "sqoop import --connect \"jdbc:mysql://localhost/retail_db\" --password admin --username root --table orders --fields-terminated-by \"\\t\" --target-dir /user/cloudera/practice3/problem3/orders\n",
    "sqoop import --connect \"jdbc:mysql://localhost/retail_db\" --password admin --username root --table order_items --fields-terminated-by \"\\t\" --target-dir /user/cloudera/practice3/problem3/order_items\n",
    "sqoop import --connect \"jdbc:mysql://localhost/retail_db\" --password admin --username root --table customers --fields-terminated-by \"\\t\" --target-dir /user/cloudera/practice3/problem3/customers\n",
    " \n",
    "#load dataset\n",
    "orders = spark.read.option(\"sep\",\"\\t\").csv(\"/user/cloudera/practice3/problem3/orders\")\n",
    "orderItems = spark.read.option(\"sep\",\"\\t\").csv(\"/user/cloudera/practice3/problem3/order_items\")\n",
    "customers = spark.read.option(\"sep\",\"\\t\").csv(\"/user/cloudera/practice3/problem3/customers\")\n",
    "\n",
    "#rename columns and filter data\n",
    "ordersRenamed = orders.selectExpr(\"_c0 as order_id\", \"_c2 as customer_id\", \"_c3 as order_status\")\n",
    "orderItemsRenamed = orderItems.selectExpr(\"_c0 as order_item_id\", \"_c1 as order_id\", \"_c4 as order_item_subtotal\")\n",
    "customersRenamed = customers.selectExpr(\"_c0 as customer_id\",\"_c6 as customer_city\")\n",
    "ordersF = ordersRenamed.filter(~ ordersRenamed.order_status.like(\"%PENDING%\"))\n",
    "\n",
    "#join dataframes and run spark sql queries\n",
    "o_oi_join = ordersF.join(orderItemsRenamed, \"order_id\")\n",
    "o_oi_join.createOrReplaceTempView(\"joins\")\n",
    "joinsF = spark.sql(\"select customer_id, sum(order_item_subtotal) as order_amount from joins group by customer_id having sum(order_item_subtotal) > 100\")\n",
    "\n",
    "result = joinsF.join(customersRenamed,\"customer_id\")\n",
    "result.createOrReplaceTempView(\"res\")\n",
    "final = spark.sql(\"select customer_city, count(*) as count from res group by customer_city\")\n",
    "\n",
    "#save to Hive\n",
    "final.write.option(\"sep\",\"\\t\").option(\"compression\",\"deflate\").format(\"csv\").save(\"/user/cloudera/practice3/problem3/joinResults\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 3 - Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instructions`\n",
    "- Input file is provided at below HDFS Location /user/cloudera/problem4_ques6/input\n",
    "- Save the data to hdfs using no compression as sequence file.\n",
    "\n",
    "\n",
    "`Output Requirement`\n",
    "- Result should be saved in at /user/cloudera/problem4_ques6/output\n",
    "- fields should be seperated by pipe delimiter.\n",
    "- Key should be order_id, value should be all fields seperated by a pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into HDFS\n",
    "sqoop import --connect \"jdbc:mysql://localhost/retail_db\" --password cloudera --username root --table orders --target-dir /user/cloudera/problem4_ques6/input --as-parquetfile\n",
    "\n",
    "#Read dataframe\n",
    "orders = spark.read.format(\"parquet\").load(\"/user/cloudera/problem4_ques6/input\")\n",
    "\n",
    "#add pipeline delimiters\n",
    "ordersMap = orders.rdd.map(lambda line: \"|\".join([str(x) for x in line]))\n",
    "\n",
    "#save to HDFS\n",
    "ordersMap.map(lambda x: (None,x)).saveAsSequenceFile(\"/user/cloudera/problem4_ques6/output\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 3 - Question5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instructions`\n",
    "- using product_ranked_new metastore table, Find the top 5 most expensive products within each category \n",
    "\n",
    "\n",
    "`Output Requirement`\n",
    "- Output should have product_category_id,product_name,product_price.\n",
    "- Result should be saved in /user/cloudera/pratice4/question2/output\n",
    "- Output should be saved as pipe delimited file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into Hive\n",
    "sqoop import --connect \"jdbc:mysql://localhost/retail_db\" --username root --password cloudera --table products --warehouse-dir /user/cloudera/practice5.db --hive-import --create-hive-table --hive-database default --hive-table product_ranked_new -m 1\n",
    "\n",
    "#run spark sql queries\n",
    "TopFiveProducts = spark.sql(\"product_category_id, product_name, product_price, dense_rank() over (partition by product_category_id order by product_price desc) as rank from product_ranked_new\")\n",
    "\n",
    "#filter dataframe\n",
    "TopFiveProductsFiltered = TopFiveProducts.filter(TopFiveProducts.rank < 6).drop(\"rank\")\n",
    "\n",
    "#add pipeline delimiters\n",
    "result = TopFiveProductsFiltered.map(lambda line: \"|\".join([str(x) for x in line]))\n",
    "\n",
    "#save to HDFS\n",
    "result.write.format(\"text\").save(\"/user/cloudera/practice4/question2/output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 3- Question6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instructions`\n",
    "- Input file is provided at below HDFS location /user/cloudera/prac3/ques6/xml\n",
    "- Append first three character of firstname with first two character of last name with a colon.\n",
    "\n",
    "\n",
    "`Output Requirement`\n",
    "- Output should be saved in xml file with rootTag as persons and rowTag as person.\n",
    "- Output should be saved at below HDFS location /user/cloudera/prac3/ques6/output/xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into HDFS\n",
    "sqoop import --connect \"jdbc:mysql://localhost/retail_db\" --password admin --username root --table customers  --target-dir /user/cloudera/prac3/ques6/xml\n",
    "\n",
    "#read and rename dataframe\n",
    "customers = spark.read.format(\"csv\").load(\"/user/cloudera/prac3/ques6/xml\")\n",
    "customersRN = customers.selectExpr(\"_c1 as fname\", \"_c2 as lname\")\n",
    "\n",
    "#run spark SQL query\n",
    "customersRN.createOrReplaceTempView(\"res\")\n",
    "\n",
    "result = spark.sql(\"select concat(fname,':',lname) as fullname from res\")\n",
    "\n",
    "#save to HDFS\n",
    "result.write.format(\"com.databricks.spark.xml\").option(\"rootTag\",\"persons\").option(\"rowTag\",\"persons\").save(\"/user/cloudera/prac3/ques6/output/xml\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
