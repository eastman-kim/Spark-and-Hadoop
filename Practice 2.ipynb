{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 1 - Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instructions`\n",
    "- Convert avro data-files stored at hdfs location /user/cloudera/practice1/problem7/customer/avro  into tab delimited file\n",
    "\n",
    "\n",
    "\n",
    "`Output Requirement`\n",
    "\n",
    "- Result should be saved in /user/cloudera/practice1/problem7/customer_text_bzip2\n",
    "- Output file should be saved as tab delimited file in bzip2 Compression.\n",
    "- Output should consist of customer_id, customer_fname(only first three letter), customer_lname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data as dataframe\n",
    "customers = spark.read.format(\"avro\").load(\"/user/cloudera/practice1/problem/customers/avro\")\n",
    "\n",
    "#create temp view\n",
    "customers.createorReplaceTempView(\"customers\")\n",
    "\n",
    "#run the sql query\n",
    "customersFiltered = spark.sql(\"select concat(customer_id,\"\\t\", substr(customer_fname,0,3),\"\\t\", customer_lname) from customers\")\n",
    "                            \n",
    "#save the dataframe\n",
    "customersFiltered.write.option(\"compression\",\"bzip2\").format(\"text\").save(\"/user/cloudera/practice1/problem7/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 1 - Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instructions`\n",
    "- Get products from metastore table named \"product_replica\" whose price > 100 and save the results in HDFS in parquet format.\n",
    "\n",
    "\n",
    "\n",
    "`Output Requirement`\n",
    "- Result should be saved in /user/cloudera/practice1/problem8/product/output as parquet file\n",
    "- Files should be saved in uncompressed format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from product_replica where product_price>100\").write.option(\"compression\",\"uncompressed\").format(\"parquet\")\\\n",
    ".save(\"/user/cloudera/practice1/problem8/product/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 1 - Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instructions`\n",
    "- Find out all PENDING_PAYMENT orders in March 2014.\n",
    "- order_date format is in unix_timestamp\n",
    "- Input file is parquet file stored at hdfs location /user/cloudera/practice1/question8\n",
    "\n",
    "\n",
    "`Output Requirement`\n",
    "- Output should be date and total pending order for that date.\n",
    "- Output should be saved at below hdfs location /user/cloudera/practice1/question8/output\n",
    "- Output should be json file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data as dataframe\n",
    "orders = spark.read.format(\"parquet\").load(\"/user/cloudera/practice1/problem8/\")\n",
    "\n",
    "#filter data\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "ordersFiltered = orders.withColumn(\"order_date\", to_date(from_unixtime(orders.order_date/1000)))\n",
    "ordersFilteredYear = ordersFiltered.filter(orders.order_date.like(\"2014-03%\"))\n",
    "ordersFilteredStatus = ordersFilteredYear.filter(orders.order_status==\"PENDING_PAYMENT\")\n",
    "\n",
    "#create temp view\n",
    "ordersFilteredStatus.createorReplaceTempView(\"ordersFiltered\")\n",
    "\n",
    "#run the sql query\n",
    "ordersGroupByOrderDate = spark.sql(\"select order_date, count(*) as count from ordersFiltered group by order_date\")\n",
    "\n",
    "#save the dataframe\n",
    "ordersGroupByOrderDate.write.json(\"/user/cloudera/practice1/problem8/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 1 - Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instructions`\n",
    "- Find out total number of orders placed by each customers in year 2013.\n",
    "- Order status should be COMPLETE\n",
    "- order_date format is in unix_timestamp\n",
    "- Input customer & order files are stored as avro file at below hdfs location \n",
    "    - /user/cloudera/practice2/question8/orders\n",
    "    - /user/cloudera/practice2/question8/customers\n",
    "\n",
    "\n",
    "`Output Requirement`\n",
    "- Output should be stored in a hive table named \"customer_order\" with three columns customer_fname,customer_lname and orders_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data as dataframe\n",
    "orders = spark.read.format(\"avro\").load(\"/user/cloudera/practice1/problem8/orders\")\n",
    "customers = spark.read.format(\"avro\").load(\"/user/cloudera/practice1/problem8/customers\")\n",
    "\n",
    "\n",
    "#filter data\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "ordersFiltered = orders.withColumn(\"order_date\", to_date(from_unixtime(orders.order_date/1000)))\n",
    "ordersFilteredYear = ordersFiltered.filter(orders.order_date.like(\"2013%\"))\n",
    "ordersFilteredStatus = ordersFilteredYear.filter(orders.order_status==\"COMPLETE\")\n",
    "ordersFilteredFinal = ordersFilteredStatus.selectExpr(\"order_id\", \"order_date\",\"order_customer_id as customer_id\")\n",
    "customers = customers.select(\"customer_id,customer_fname\", \"customer_lname\", \"customer_state\")\n",
    "\n",
    "#create temp view\n",
    "ordersFilteredStatus.createorReplaceTempView(\"ordersFiltered\")\n",
    "\n",
    "#run the sql query\n",
    "ordersGroupByCustomerId = spark.sql(\"select customer_id, count(*) as order_count from ordersFiltered group by customer_id\")\n",
    "\n",
    "#join two datasets\n",
    "ordersCustomersJoin = ordersGroupByCustomerId.join(customers, \"customer_id\")\n",
    "result = ordersCustomersJoin.select(\"customer_fname\",\"customer_lname\",\"order_count\",\"customer_state\")\n",
    "#save the dataframe in Hive\n",
    "result.write.partitionBy(\"customer_state\").format(\"hive\").saveAsTable(\"customer_order\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
